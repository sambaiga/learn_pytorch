{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learing with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1. 1 What is Pytorch\n",
    "PyTorch is a python package that provides two high-level features: Tensor computation (like numpy) with strong GPU acceleration and Deep Neural Networks built on a tape-based autograd system <sup>[1](#myfootnote1)</sup>. These features enable pytorch to act as a replacement for numpy to use the power of GPUs and deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "Unlike Tensorfolow,  PyTorch supports creation of dynamic computation graphs (DCG), whereas Tensorflow use a static computation graph (SCG). For a clear and systematically comparison between PyTorch and TensorFlow you may refer to this [blog post](https://awni.github.io/pytorch-tensorflow/).\n",
    "\n",
    "\n",
    "### 1.2 Install pytorch\n",
    " To install pytorch follow installion procedures available in [pytorch website](http://pytorch.org/). However if your machine support GPU you need first to install NVDIA drivers by following [these instruction](). Pytorch comes with its own runtime cuda libraries, but driver has to be installed on each machine separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Pytorch Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Pytorch Tensors\n",
    "\n",
    "The main building block of the PyTorch is the tensors. So what is tensor? \n",
    "\n",
    "**Tensor** is a multi-dimensional matrix containing elements of a single data type. They are very similar to the NumPy array. However, unlike numpy array, pytorch tensor can utilize GPU.\n",
    "\n",
    "#### 2.1.1 A tensor can be constructed from a Python list or sequence with the **torch.Tensor()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a torch.Tensor object with the given data.  It is a 1D vector\n",
    "data = [1., 2., 3.]\n",
    "V = torch.Tensor(data)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  2  3\n",
      " 4  5  6\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates a matrix\n",
    "data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.Tensor(data)\n",
    "print (M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  1  2\n",
      "  3  4\n",
      "\n",
      "(1 ,.,.) = \n",
      "  5  6\n",
      "  7  8\n",
      "[torch.FloatTensor of size 2x2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a 3D tensor of size 2x2x2.\n",
    "data = [[[1.,2.], [3.,4.]],\n",
    "          [[5.,6.], [7.,8.]]]\n",
    "T = torch.Tensor(data)\n",
    "print (T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors and matrices are special cases of torch.Tensors, where their dimension is 1 and 2 respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 You can create a tensor with *random data* and the supplied dimensionality with **torch.randn()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1.9459 -1.2877 -1.5302 -1.6099\n",
      " 0.5595  2.0083 -0.2192 -0.0422\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      " -0.8481 -0.2042  1.2779 -0.6543\n",
      "  0.5905  0.3888 -1.4197 -0.6554\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.3308 -0.2200  1.5656  0.0879\n",
      "  0.1800  0.3002  1.9382 -0.5665\n",
      "[torch.FloatTensor of size 2x2x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = torch.randn(2, 2, 4)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use special tensors line ones and zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1  1\n",
       " 1  1  1\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       "[torch.FloatTensor of size 3x5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Numpy Bridge\n",
    "   \n",
    "You can easily convernt pytorh tensor into numpy array and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.38679829 -0.20805581  1.09448074 -3.23681379]\n",
      " [ 2.14710092  0.50938828  1.65233662  0.75678135]\n",
      " [ 0.58488052 -1.54270815 -1.00415993  0.04386759]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_tensor = np.random.randn(3, 4)\n",
    "print(numpy_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.3868 -0.2081  1.0945 -3.2368\n",
      " 2.1471  0.5094  1.6523  0.7568\n",
      " 0.5849 -1.5427 -1.0042  0.0439\n",
      "[torch.FloatTensor of size 3x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert numpy array to pytorch array\n",
    "pytorch_tensor = torch.Tensor(numpy_tensor)\n",
    "print(pytorch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3867983 , -0.20805581,  1.0944808 , -3.2368138 ],\n",
       "       [ 2.147101  ,  0.50938827,  1.6523366 ,  0.75678134],\n",
       "       [ 0.58488053, -1.5427082 , -1.0041599 ,  0.04386758]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert torch tensor to numpy representation\n",
    "pytorch_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Operations with Tensors\n",
    "\n",
    "You can operate on tensors in the ways you would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([ 1., 2., 3. ])\n",
    "y = torch.Tensor([ 4., 5., 6. ])\n",
    "z = x + y\n",
    "print (z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can also use\n",
    "z = torch.add(x, y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping Tensors\n",
    "\n",
    "The **.view()** method  provide a function to reshape a tensor. This method receives heavy use, because many neural network components expect their inputs to have a certain shape. Often you will need to reshape before passing your data to the component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -0.4414  0.4427  0.9195  0.7832\n",
       " -0.4930  1.3495  0.6200  0.7397\n",
       " -0.3259 -0.4232 -0.0343  0.4567\n",
       "[torch.FloatTensor of size 1x3x4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 9 \n",
       "-0.4414  0.4427  0.9195  0.7832 -0.4930  1.3495  0.6200  0.7397 -0.3259 -0.4232\n",
       "\n",
       "Columns 10 to 11 \n",
       "-0.0343  0.4567\n",
       "[torch.FloatTensor of size 1x12]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape to 1 rows, 12 columns\n",
    "x.view(1, 12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -0.4414  0.4427\n",
       "  0.9195  0.7832\n",
       " -0.4930  1.3495\n",
       "  0.6200  0.7397\n",
       " -0.3259 -0.4232\n",
       " -0.0343  0.4567\n",
       "[torch.FloatTensor of size 1x6x2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape to 1x6x2 \n",
    "x.view(1, 6, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Autograd and Variables \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autograd** provide a mechanism to compute error gradients and back-propagated through the computational graph. The **Variable class** is the main component of this autograd system in PyTorch.\n",
    "\n",
    "**Variables** are wrappers above tensors and construct a chain of operations between tensors.They are like placeholders in Tensorflow. Variables are useful when building  computational graph, and computing gradients automatically. \n",
    "\n",
    "Every variable instance has two attributes: **.data** that contain initial tensor itself and **.grad** that will contain gradients for the corresponding tensor. Unlike TensorFlow’s, PyTorch Variable will have data in it. **Autograd** allows you to automatically compute gradients of tensor variable.\n",
    "\n",
    "###NOTE:\n",
    "**Computation graph** is simply a specification of how your data is combined to give you the output. Since the graph totally specifies what parameters were involved with which operations, it contains enough information to compute derivatives. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example: if we have $y = wx + b$ it clear that $\\frac{\\partial y}{\\partial x} =w$, $\\frac{\\partial y}{\\partial b} = 1$ and $\\frac{\\partial y}{\\partial w} = x$\n",
    "\n",
    "\n",
    "To compute the derivatives, you can call **.backward()** on a Variable. If Variable is a scalar (i.e. it holds a one element tensor), you don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a grad_output argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "# Create tensors.\n",
    "x = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "w = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "b = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 18\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create tensors variables.\n",
    "x = Variable(torch.ones(1, 1), requires_grad=True) \n",
    "\n",
    "# perform operations\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "\n",
    "# find gradient\n",
    "z.backward()\n",
    "\n",
    "#print gradient\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of x is equal to 18. This is equivalent to:\n",
    "$$\n",
    "z = 3y^2 \\text{ where } y = x + 2 \\Rightarrow z = 3(x + 2)^2\n",
    "$$\n",
    "\n",
    "Thus: $$ \\frac{dz}{dx} = 6(x +2) = 6(1+2) = 18$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Deep Learning Building Blocks\n",
    "\n",
    "Deep learning consists of composing linearities with non-linearities modules. The introduction of non-linearities allows for powerful models. Given linear and non-liear module how to define objective function and train deep learninh model in pytorch.\n",
    "\n",
    "Neural networks can be constructed using the **torch.nn** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linear function (Affine Maps)\n",
    "\n",
    "This is the core building block of deep learning defined is a function:\n",
    "$$ f(x) = \\mathbf{wx + b}$$ for a matrix $\\mathbf{w} $ and vectors $\\mathbf{x,b}$. Linear function is implemented in: torch.nn\n",
    "\n",
    "**torch.nn.Linear(in_features, out_features, bias=True)**\n",
    "\n",
    "\n",
    "Note: pytorch maps the rows of the input instead of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.9201\n",
      "-0.8693\n",
      "-0.8491\n",
      "-0.6123\n",
      "-1.1473\n",
      "-0.9041\n",
      "-0.7474\n",
      "-0.9685\n",
      "-0.6398\n",
      "-0.7985\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(2, 1, bias=True)\n",
    "data = Variable(torch.rand(10, 2))\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Non-Linearities Function (Activation Function)\n",
    "\n",
    "Most used non-linear functions are: sigmoid, tanh and relu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.3890  0.5418\n",
      " 0.1263  0.4530\n",
      " 0.7522  0.4513\n",
      " 0.1511  0.0656\n",
      " 0.5568  0.8930\n",
      " 0.5517  0.5252\n",
      " 0.0433  0.2650\n",
      " 0.8734  0.6374\n",
      " 0.5678  0.1264\n",
      " 0.2168  0.3501\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(F.relu(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.5960  0.6322\n",
      " 0.5315  0.6114\n",
      " 0.6797  0.6109\n",
      " 0.5377  0.5164\n",
      " 0.6357  0.7095\n",
      " 0.6345  0.6284\n",
      " 0.5108  0.5659\n",
      " 0.7054  0.6542\n",
      " 0.6383  0.5316\n",
      " 0.5540  0.5866\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(F.sigmoid(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a neural network\n",
    "\n",
    "To create a neural network in PyTorch, we use **nn.Module** base class with Python class inheritance which allows us to use all of the functionality of the **nn.Module base class**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, nb_feature, nb_output):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(nb_feature, nb_output)  \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the class definition, you can see the inheritance of the base class **torch.nn.Module**. \n",
    "- Then, in the first line of the class initialization (def __init__(self):) we have the required Python **super() function**, which creates an instance of the base **torch.nn.Module** class. \n",
    "- The next line define a linear object defined by **torch.nn.Linear**, with the first argument in the definition being the number of input feature and the next argument being the number of output.\n",
    "- After that we need to define how data flows through out network. This can be doe using **forward()** method in which we supply the input data x as the primary argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create an instance of this network architecture and assign this instance to cuda() method if available. Suppose we have the following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors.\n",
    "x = Variable(torch.randn(10, 2))\n",
    "y = Variable(torch.randn(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(2, 1)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the instance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (linear): Linear(in_features=2, out_features=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "To train this model we need to setup an optimizer and a loss criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the first line, we create a stochastic gradient descent optimizer, and we specify the learning rate and supply the model parameters using **model.parameters()** method of the base **torch.nn.Module** class that we inherit.\n",
    "- Next, we set our loss criterion to be the **MSE** loss. For details on different loss function you may refer to [pytorch documentation](http://pytorch.org/docs/master/nn.html#loss-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the training process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we run optimizer.zero_grad() – this zeroes / resets all the gradients in the model, so that it is ready to go for the next back propagation pass. In other libraries this is performed implicitly, but in PyTorch you have to remember to do it explicitly.\n",
    "- Then we we pass the input data into the model **pred = model(x)** – this will call the **forward()** method in our model class.\n",
    "- After that we get the MSE loss between the output of our network and the target data as **loss = criterion(y_pred, y_data)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  12.298934936523438\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "pred = model(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then we runs a back-propagation operation from the loss Variable backwards through the network using **loss.backward()***\n",
    "- Finaly we tell PyTorch to execute a gradient descent step based on the gradients calculated during the **.backward()** operation using **optimizer.step()**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "-[Adventures in machine learning](http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/)\n",
    "-[DeepLearningZeroToAll](https://github.com/hunkim/DeepLearningZeroToAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"myfootnote1\">1</a>: http://pytorch.org/about/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
